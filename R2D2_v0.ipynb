{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, os, sys, gc\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "%matplotlib inline\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    # tf.config.experimental.set_virtual_device_configuration(gpu, [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=0.95)])\n",
    "\n",
    "\n",
    "from keras import layers, models, backend\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import get_custom_objects\n",
    "\n",
    "# import tensorflow_addons as tfa\n",
    "# import tensorflow_probability as tfp\n",
    "\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# from keras import mixed_precision\n",
    "# mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_layers import *\n",
    "from custom_encodingdecoding import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - One Model To Rule Them All, ... And Within Its Latents Bind Them - Recurisive Recontextualization and Dynamic Distillation (R2D2)\n",
    "#\n",
    "# Project Description:\n",
    "# A dynamically recursive and scalable multi-modal neural network architecture.\n",
    "# Dynamically recursive in the sense that the model operates off of a singular recursive block, \n",
    "# wherin a hybrid processing block creates an intermediate view from the data patches, combined with a view from the working context,\n",
    "# to calculate updates to the working context,  then block weights are updated, and we use intermediates and the context to predict \n",
    "# an update to the data patches, and the process repeats.\n",
    "# a positional embedding is used to add positional information to the data patches, but additionally we use a type embedding to \n",
    "# help the model differentiate between types of data patches, and additionally to identify target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class R2D2(tf.keras.models.Model):\n",
    "    def __init__(self, data_size, context_size, head_width, weight_gen_size, modes, depth, activation, encoder_decoder_params):\n",
    "        super(R2D2, self).__init__()\n",
    "        self.data_size = data_size\n",
    "        self.context_size = context_size\n",
    "        self.head_width = head_width\n",
    "        self.weight_gen_size = weight_gen_size\n",
    "        self.modes = modes\n",
    "        self.depth = depth\n",
    "        self.act = activation\n",
    "        self.encoder_decoder_params = encoder_decoder_params\n",
    "\n",
    "        self.data_encoders, self.data_decoders = get_encoders_and_decoders(self.data_size, self.modes, **self.encoder_decoder_params)\n",
    "\n",
    "        self.num_heads = self.context_size // self.head_width\n",
    "\n",
    "        self.att_kernel_size = 7 # should possibly increase depending on internal sizes\n",
    "        \n",
    "\n",
    "        # positional encoding stack\n",
    "        self.wpos = []\n",
    "        scale = 1\n",
    "        while self.num_heads // scale >= 2:\n",
    "            self.wpos.append(self.add_weight(name='widthwise_positional_encoding_h' + str(self.num_heads // scale), \n",
    "                                             shape=(1, self.head_width * (self.num_heads // scale)), \n",
    "                                             initializer='random_normal', trainable=True))\n",
    "            scale *= 2\n",
    "        self.wpos.append(self.add_weight(name='widthwise_positional_encoding_h1', shape=(1, self.head_width), initializer='random_normal', \n",
    "                                      trainable=True))\n",
    "        \n",
    "        # locality embeddings\n",
    "        self.data_locality_dense = layers.Dense(64, activation=self.act, name='data_locality_dense')\n",
    "        self.data_locality_kernel = layers.Dense(self.att_kernel_size, activation='linear', name='data_locality_kernel', use_bias=False)\n",
    "        \n",
    "        # type embedding\n",
    "        self.type_embedding = layers.Embedding(len(self.modes), self.context_size, name='type_embedding')\n",
    "\n",
    "        # context initializer\n",
    "        self.context_initializer = self.add_weight(name='context_initializer', shape=(1, self.context_size), initializer='random_normal', trainable=True)\n",
    "\n",
    "        # block weights\n",
    "\n",
    "        self.weight_shapes = {}\n",
    "\n",
    "        self.weight_shapes['data_view'] = (self.data_size, self.data_size)\n",
    "        self.weight_shapes['context_view'] = (self.context_size, self.data_size)\n",
    "\n",
    "        self.weight_shapes['context_update'] = (self.data_size, self.context_size)\n",
    "        # self.weight_shapes['context_update_importance'] = (self.data_size, 2)\n",
    "\n",
    "        self.weight_shapes['context_view_2'] = (self.context_size, self.data_size)\n",
    "        self.weight_shapes['data_update'] = (self.data_size, self.data_size)\n",
    "        # self.weight_shapes['data_update_importance'] = (self.data_size, 2)\n",
    "\n",
    "        # weight generation\n",
    "\n",
    "        # self.list_of_weight_shapes = [tf.cast(w, tf.int32) for w in list(self.weight_shapes.items())]\n",
    "        self.list_of_weight_shapes = [\n",
    "            (self.data_size, self.data_size),\n",
    "            (self.context_size, self.data_size),\n",
    "            (self.data_size, self.context_size),\n",
    "            (self.context_size, self.data_size),\n",
    "            (self.data_size, self.data_size)\n",
    "        ]\n",
    "        self.weight_lens = [tf.math.reduce_prod(w) for w in self.list_of_weight_shapes]\n",
    "        total_weights = tf.math.reduce_sum(self.weight_lens)\n",
    "\n",
    "        self.weight_gen_comp = layers.Dense(self.weight_gen_size, activation=self.act, name='weight_gen_comp')\n",
    "        self.weight_gen = layers.Dense(total_weights, activation='linear', name='weight_gen')\n",
    "\n",
    "        # weight norms\n",
    "        self.weight_norms = [layers.LayerNormalization(name='weight_norm_' + str(i)) for i in range(len(self.list_of_weight_shapes))]\n",
    "\n",
    "        self.inverse_weight_proc = layers.Dense(self.context_size, activation=self.act, name='inverse_weight_proc')\n",
    "\n",
    "    # aggregate the positional encodings\n",
    "    def get_positional_encoding(self):\n",
    "        positional_encoding = self.wpos[0]\n",
    "        for i in range(1, len(self.wpos)):\n",
    "            wpos = self.wpos[i]\n",
    "            # repeat the positional encoding as many times as needed\n",
    "            num_repeats = tf.cast(tf.math.ceil(self.context_size / tf.shape(wpos)[1]), tf.float32)\n",
    "            num_repeats_int = tf.cast(num_repeats, tf.int32)\n",
    "            wpos = tf.repeat(wpos, num_repeats_int, axis=0)\n",
    "            # multiply by linear decay\n",
    "            # print(wpos.shape, (tf.cast(tf.range(num_repeats_int, 0, -1), tf.float32) / num_repeats).shape)\n",
    "            wpos = wpos * tf.expand_dims(tf.cast(tf.range(num_repeats_int, 0, -1), tf.float32) / num_repeats, axis=-1)\n",
    "            # unstack the repeats\n",
    "            wpos = tf.reshape(wpos, (-1,))\n",
    "            # truncate to the correct length and add to the positional encoding\n",
    "            positional_encoding = positional_encoding + wpos[:self.context_size]\n",
    "        return positional_encoding\n",
    "    \n",
    "    def get_locality_embedding(self, patch_dims):\n",
    "        x = self.data_locality_dense(patch_dims)\n",
    "        k = self.data_locality_kernel(x)\n",
    "        # reshape k into a kernel\n",
    "        k = tf.reshape(k, (self.att_kernel_size, 1, -1))\n",
    "        # apply the kernel to the positional encoding\n",
    "        pos_enc = tf.expand_dims(self.get_positional_encoding(), axis=-1)\n",
    "        output = tf.nn.conv1d(pos_enc, k, stride=1, padding='SAME')\n",
    "        # transpose the output to the correct shape\n",
    "        output = tf.transpose(output, (0, 2, 1))\n",
    "        return output[0] # should fix batch size elsewhere\n",
    "    \n",
    "    # weight generation\n",
    "    def get_weights(self, context):\n",
    "        # normalize the context (need to handle zero case)\n",
    "        # context = (context - tf.math.reduce_mean(context, axis=0, keepdims=True)) / tf.math.reduce_std(context, axis=0, keepdims=True)\n",
    "        intermediate = self.weight_gen_comp(context)\n",
    "        weights = self.weight_gen(intermediate)\n",
    "\n",
    "        # split the weights\n",
    "        weights = tf.split(weights, self.weight_lens, axis=-1)\n",
    "        weights = [tf.reshape(w, s) for w, s in zip(weights, self.list_of_weight_shapes)]\n",
    "        weights = [self.weight_norms[i](w) for i, w in enumerate(weights)]\n",
    "\n",
    "        # return as a dictionary with names\n",
    "        return dict(zip(self.weight_shapes.keys(), weights))\n",
    "    \n",
    "    def change_depth(self, new_depth):\n",
    "        self.depth = new_depth\n",
    "\n",
    "    def process_recursive_step(self, X, E, YS, Y, context, training=False, inverse=False):\n",
    "        # X is the input data (batch size, data size)\n",
    "        # E is the locality embedding (batch size, context size)\n",
    "        # Y is the y selection (batch size, 1)\n",
    "        # context is the context (batch size, context size)\n",
    "\n",
    "        # generate weights\n",
    "        if inverse:\n",
    "            wcontext = self.inverse_weight_proc(context)\n",
    "        else:\n",
    "            wcontext = context\n",
    "        weights = self.get_weights(wcontext)\n",
    "\n",
    "        # initial data view\n",
    "        xX = X\n",
    "        cC = context\n",
    "\n",
    "        # apply weights\n",
    "        xV = tf.matmul(xX + tf.gather(E, tf.range(self.data_size), axis=-1), weights['data_view'])\n",
    "        cV = tf.matmul(context + E, weights['context_view'])\n",
    "\n",
    "        V = xV + cV\n",
    "        V = self.act(V)\n",
    "\n",
    "        cU = tf.matmul(V, weights['context_update'])\n",
    "        cU = tf.math.reduce_mean(cU, axis=0, keepdims=True)\n",
    "        # normalize the context update\n",
    "        # cU = (cU - tf.math.reduce_mean(cU, axis=-1, keepdims=True)) / (tf.math.reduce_std(cU, axis=-1, keepdims=True) + 1e-3)\n",
    "        cC = cC + cU\n",
    "        # normalize the context\n",
    "        cC = (cC - tf.math.reduce_mean(cC, axis=-1, keepdims=True)) / (tf.math.reduce_std(cC, axis=-1, keepdims=True) + 1e-3)\n",
    "\n",
    "        ydx = tf.reshape(tf.where(YS > 0.0), (-1,))\n",
    "        cV2 = tf.matmul(cC + (tf.gather(E, ydx, axis=0)), weights['context_view_2'])\n",
    "        V2 = tf.gather(V, ydx, axis=0) + cV2\n",
    "        V2 = self.act(V2)\n",
    "\n",
    "        # apply data update\n",
    "        dU = tf.matmul(V2, weights['data_update'])\n",
    "        # normalize the data update\n",
    "        # dU = (dU - tf.math.reduce_mean(dU, axis=-1, keepdims=True)) / (tf.math.reduce_std(dU, axis=-1, keepdims=True) + 1e-3)\n",
    "        uX = tf.gather(xX, ydx, axis=0) + dU\n",
    "        # normalize the data\n",
    "        uX = (uX - tf.math.reduce_mean(uX, axis=-1, keepdims=True)) / (tf.math.reduce_std(uX, axis=-1, keepdims=True) + 1e-3)\n",
    "\n",
    "        # scatter the updated data\n",
    "        # print(X.shape, ydx.shape, uX.shape)\n",
    "        X = tf.tensor_scatter_nd_update(X, tf.expand_dims(ydx, axis=-1), uX)\n",
    "\n",
    "        loss = None\n",
    "        if training:\n",
    "            loss = tf.math.reduce_mean(tf.math.square(uX - Y))\n",
    "\n",
    "        return X, context, loss\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x, y, mode = inputs\n",
    "        # x is input data (batch size, z length, *(data size, differs by mode))\n",
    "        # y represents what z steps are being predicted (batch size, z) where each z scales from 0 to 1, 0 if not predicted, 1 if predicted\n",
    "        # if training, we use y to mask/corrupt corresponding x data for autorregressive training,\n",
    "        # otherwise we use it to determine what z steps to predict\n",
    "        # mode determines how z steps are encoded and decoded. (batch size, z)\n",
    "\n",
    "        # assert len(x) == len(y) == len(mode)\n",
    "\n",
    "        # since batches and z steps are generally ragged, we're just going to encode them separately for now\n",
    "        bs = len(x) # tf.shape(x)[0]\n",
    "        \n",
    "        FX = []\n",
    "        EE = []\n",
    "        PS = []\n",
    "        YS = []\n",
    "        dec_loss = 0.0\n",
    "        for b in range(bs):\n",
    "            bx = x[b]\n",
    "            by = y[b]\n",
    "            bmode = mode[b]\n",
    "            fx = []\n",
    "            ee = []\n",
    "            ps = []\n",
    "            ys = []\n",
    "            for z in range(len(bx)): # range(tf.shape(bx)[0]):\n",
    "                ex, p_s, ped = self.data_encoders[bmode[z]]([bx[z], z - len(bx)]) # tf.shape(bx)[0]])\n",
    "                # normalize ex\n",
    "                # ex = (ex - tf.math.reduce_mean(ex, axis=-1, keepdims=True)) / (tf.math.reduce_std(ex, axis=-1, keepdims=True) + 1e-3)\n",
    "                if training: # decode the data and compare to the original\n",
    "                    dex = self.data_decoders[bmode[z]]([ex, p_s])\n",
    "                    dec_loss += tf.math.reduce_mean(tf.math.square(dex - bx[z])) * (1.0 / len(bx))\n",
    "\n",
    "                # use ped to get the locality embedding\n",
    "                locality_embedding = self.get_locality_embedding(ped)\n",
    "\n",
    "                # we also need to copy by[z] by len of ex for selecting updates later\n",
    "                byz = tf.repeat(tf.expand_dims(by[z], axis=0), tf.shape(ex)[0], axis=0)\n",
    "\n",
    "                # get type embedding, check bmode[z] against self.modes\n",
    "                ste = tf.where(tf.equal(bmode[z], self.modes))[0][0]\n",
    "                ste = self.type_embedding(ste)\n",
    "                # expand ste to the length of ex\n",
    "                ste = tf.repeat(tf.expand_dims(ste, axis=0), tf.shape(ex)[0], axis=0)\n",
    "\n",
    "                fx.append(ex)\n",
    "                ee.append(locality_embedding + ste)\n",
    "                ps.append(p_s)\n",
    "                ys.append(byz)\n",
    "\n",
    "            FX.append(tf.concat(fx, axis=0))\n",
    "            EE.append(tf.concat(ee, axis=0))\n",
    "            PS.append(tf.concat(ps, axis=0))\n",
    "            YS.append(tf.concat(ys, axis=0))\n",
    "\n",
    "        FX = tf.stack(FX, axis=0)\n",
    "        EE = tf.stack(EE, axis=0)\n",
    "        PS = tf.stack(PS, axis=0)\n",
    "        YS = tf.stack(YS, axis=0)\n",
    "\n",
    "        FY = None\n",
    "        if training:\n",
    "            FY = tf.gather(FX, tf.reshape(tf.where(YS > 0.0), (-1,)), axis=0)\n",
    "        mYS = tf.expand_dims(YS, axis=-1)\n",
    "        FX = (FX * (1.0 - mYS)) + (tf.zeros(tf.shape(FX), dtype=tf.float32) * (mYS))\n",
    "\n",
    "        # FX is the encoded data (batch size * (*variable), data size)\n",
    "        # LE is the locality embedding (batch size * (*variable), context size)\n",
    "        # PS is the positional encoding (batch size * zsteps, (*variable))\n",
    "        # YS is the y selection (batch size * (*variable), 1)\n",
    "\n",
    "        # initialize the context\n",
    "        context = self.context_initializer\n",
    "\n",
    "        gtloss = 0.0\n",
    "        giloss = 0.0\n",
    "        for i in range(self.depth):\n",
    "            # apply the recursive step\n",
    "            nX, nContext, nloss = self.process_recursive_step(FX, EE, YS, FY, context, training=training)\n",
    "            if training:\n",
    "                # inverse step\n",
    "                iFY = tf.gather(FX, tf.reshape(tf.where(YS > 0.0), (-1,)), axis=0)\n",
    "                iX, iContext, iloss = self.process_recursive_step(nX, EE, YS, iFY, nContext * -1, training=training, inverse=True)\n",
    "\n",
    "                if i >= 2: # min recursion for loss\n",
    "                    gtloss += nloss * (1.0 / (self.depth - 2))\n",
    "                if i < self.depth - 2:\n",
    "                    giloss += iloss * (1.0 / (self.depth - 2))\n",
    "            FX = nX\n",
    "            context = nContext\n",
    "\n",
    "\n",
    "        if training:\n",
    "            gloss = gtloss + giloss + dec_loss\n",
    "            return FX, [gloss, gtloss, giloss, dec_loss]\n",
    "        else:\n",
    "            return FX, [None, None, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inds = 2560\n",
    "dpth = 8\n",
    "test_model = R2D2(\n",
    "    data_size=1024, \n",
    "    context_size=4096, \n",
    "    head_width=128, # is this even used?\n",
    "    weight_gen_size=dpth, \n",
    "    modes=['image', 'imagenet1k_classification'], \n",
    "    depth=dpth,\n",
    "    activation=aptx,\n",
    "    encoder_decoder_params={\n",
    "        'patch_size':16,\n",
    "        'conv_mult':1.0,\n",
    "        'vocab_length':1024,\n",
    "        'char_embed_size':16,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow_datasets as tfds\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cache_dir='S:/Datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet1k = load_dataset('imagenet-1k', cache_dir=data_cache_dir, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(imagenet1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = tf.expand_dims(tf.image.resize_with_crop_or_pad(tf.keras.utils.img_to_array(imagenet1k[0]['image']) / 255.0, 256, 256), axis=0)\n",
    "test_class = tf.one_hot(imagenet1k[0]['label'], 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(([[test_image, test_class]], [[0., 1.]], [['image', 'imagenet1k_classification']]), training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0003, beta_1=0.9, beta_2=0.95, epsilon=1e-07, amsgrad=False, name='Adam', clipnorm=0.1)\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True, name='SGD', clipnorm=0.1)\n",
    "# loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "loss_fn = lambda x: x\n",
    "\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "    for step in range(len(imagenet1k)):\n",
    "        x_batch_train = tf.expand_dims(tf.keras.utils.img_to_array(imagenet1k[step]['image']) / 255.0, axis=0)\n",
    "        im_x = x_batch_train.shape[-3]\n",
    "        im_y = x_batch_train.shape[-2]\n",
    "        if im_x > im_y:\n",
    "            x_batch_train = tf.image.resize(x_batch_train, (int((im_x / im_y) * 256), 256))\n",
    "        else:\n",
    "            x_batch_train = tf.image.resize(x_batch_train, (256, int((im_y / im_x) * 256)))\n",
    "        x_batch_train = tf.image.resize_with_crop_or_pad(x_batch_train, 256, 256)\n",
    "        y_batch_train = tf.one_hot(tf.convert_to_tensor(imagenet1k[step]['label']), 1000)\n",
    "\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits, losses = test_model(([[x_batch_train, y_batch_train]], [[0.0, 1.0]], [['image', 'imagenet1k_classification']]), training=True) \n",
    "\n",
    "            loss_value = losses[0]\n",
    "\n",
    "            cat_preds = test_model.data_decoders['imagenet1k_classification']([logits[:,-1], [1,]])[0]\n",
    "            cat_loss = tf.keras.losses.categorical_crossentropy(y_batch_train, cat_preds)\n",
    "            loss_value = (cat_loss * 0.1) + losses[1] + losses[2] + (losses[3] * 0.1)\n",
    "\n",
    "        grads = tape.gradient(loss_value, test_model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, test_model.trainable_weights))\n",
    "\n",
    "        print(\n",
    "            \"Training losses (idv) at step %d:\"\n",
    "            % (step,), [np.round(float(loss_value), 8)] + \n",
    "            [np.round(float(loss), 8) for loss in losses[1:]] + \n",
    "            [np.round(float(cat_loss), 8)], \"Seen: %s samples\" % ((step + 1) * batch_size)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet1k_valid = load_dataset('imagenet-1k', cache_dir=data_cache_dir, split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate the model\n",
    "\n",
    "agg_loss = 0.0\n",
    "agg_acc = 0.0\n",
    "\n",
    "for step in range(len(imagenet1k_valid)):\n",
    "    istep = step\n",
    "    x_batch = tf.expand_dims(tf.keras.utils.img_to_array(imagenet1k_valid[istep]['image']) / 255.0, axis=0)\n",
    "\n",
    "    im_x = x_batch.shape[-3]\n",
    "    im_y = x_batch.shape[-2]\n",
    "    if im_x > im_y:\n",
    "        x_batch = tf.image.resize(x_batch, (int((im_x / im_y) * 256), 256))\n",
    "    else:\n",
    "        x_batch = tf.image.resize(x_batch, (256, int((im_y / im_x) * 256)))\n",
    "    x_batch = tf.image.resize_with_crop_or_pad(x_batch, 256, 256)\n",
    "    y_batch = tf.one_hot(tf.convert_to_tensor(imagenet1k_valid[istep]['label']), 1000)\n",
    "\n",
    "    y_zero = tf.zeros_like(y_batch, dtype=tf.float32) + (1.0 / 1000.0)\n",
    "    logits, losses = test_model(([[x_batch, y_zero]], [[0.0, 1.0]], [['image', 'imagenet1k_classification']]), training=True)  # Logits for this minibatch)\n",
    "\n",
    "    # use the decoder in the model to decode for classification\n",
    "    logits = test_model.data_decoders['imagenet1k_classification']([logits[:,-1], [1,]])\n",
    "\n",
    "    loss_value = tf.keras.losses.categorical_crossentropy(tf.expand_dims(y_batch, axis=0), logits)\n",
    "    accuracy = tf.keras.metrics.categorical_accuracy(tf.expand_dims(y_batch, axis=0), logits)\n",
    "\n",
    "    agg_loss += loss_value\n",
    "    agg_acc += accuracy\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(\"step: %d, loss: %.4f, accuracy: %.4f\" % (step, agg_loss / (step + 1), agg_acc / (step + 1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0d8f4ad52842acb1bd88b5e4d5a3e855ffe6009d6de89734aac6954d5696bda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
